> Web crawler aka. robot or spider. Được dùng bởi các search engine để tìm ra content mới trên web. Content có thể là webpage, image, video, PDF file,... Web crawler sẽ collect một số pages rồi follow theo link ở trên nó tới page khác để collect content khác.
> ![[Pasted image 20230420162547.png]]
> Web crawler được dùng cho nhiều mục đích:
> - Search engine indexing: Tạo local index cho search engine (VD: Googlebot của Google).
> - Web archiving
> - Web mining: data mining (VD một số công ty lớn dùng crawler để download các cuộc họp hay báo cáo thường niên của cổ đông,...)

Basic algorithm cho web crawler là:
1. Cho một tập các URL, download webpage từ URL đó.
2. Extract URL từ các webpage này.
3. Tiếp tục add URL vào list và lặp lại bước 1.
> Requirement (từ trong sách)
> - Dùng cho search engine indexing
> - Mỗi tháng collect 1 tỷ page
> - Chỉ lưu file HTML
> - Consider cả những page mới thêm hoặc mới update
> - Lưu trữ trong vòng 5 năm
> - Ignore những page bị duplicate

Ngoài những requirement trên, ta còn phải tuân theo những tính chất của 1 good web crawler:
- Scalability: Vì có hàng tỷ webpage trên internet nên ta phải có cách crawling sao cho hiệu quả sử dụng ***parallelization***.
- Robustness: Những edge case có thể xảy ra: Bad HTML, unresponsive servers, crash, link độc hại,... Crawler phải handle được những case này.
- Politeness: Không nên tạo quá nhiều request trong một khoảng thời gian ngắn.
- Extensibility: Phải flexible.
> Back of envelope estimation
> - Giả sử có 1 tỷ webpage được tải mỗi tháng: QPS: 1 billion / 30 days / 24 hours / 3600 secs = 400 pages/second
> - Peak QPS = QPS * 2 (thường là vậy) = 800
> - Ví dụ size của mỗi page là 500k
> - 1 billion page * 100k = 500TB / month => 500 TB x 12 months x 5 years = 30 PB.

![[Pasted image 20230420171621.png]]
# High level design
## Seed URLs
Một web crawler sử dụng seed url làm *starting point*, một good seed url giúp ta có thể traverse qua càng nhiều link càng tốt.
## URL Frontier
Hầu hết modern web crawler chia crawl state thành 2 state: cần download và đã download. Component chứa URL cần download được gọi là **URL Frontier**. URL Frontier là 1 Queue.
## HTML Downloader
## DNS Resolver
## Content Parser
Sau khi download về, web page phải được parse và validate vì website bị malform sẽ gây tốn dung lượng. Content Parser nằm chung với crawl server có thể làm chậm quá trình crawling => Là 1 component riêng biệt.
## Content Seen?
Tránh lưu những content đã được lưu rồi.
## Content Storage
- Hầu hết data được lưu trên disk
- Popular content được lưu trong memory
## URL Extractor
URL Extract sẽ parse và extract link từ HTML pages
![[Pasted image 20230421151543.png]]
## URL Filter
URL Filter dùng để exclude ra những content type, file extension, error link, URL nằm trong *blacklist*.
## URL Seen?
Là 1 data structure keep track xem những URL nào được visit hoặc nằm trong Frontier rồi. Nó giúp ta tránh add cùng URL nhiều lần vì sẽ khiến tăng server load và có thể dẫn đến infinite loop.
*Bloom Filter* và *Hash table* là những technique phổ biến để implement "URL Seen" component.
## URL Storage
Lưu những URLs đã được visited.
# Web Crawler workflow
1. Add seed URLs vào URL Frontier.
2. HTML downloader fetch danh sách URL từ URL Frontier.
3. HTML Downloader get IP address của URL từ DNS resolver và bắt đầu download.
4. Sau khi Content Parser parses HTML page và kiểm tra xem page có bị malform không.
5. Content Seen component sẽ check xem HTML page đã có trong storage chưa. Rồi thì discard HTML page. Chưa thì được đẩy tiếp tới Link Extractor.
6. Link Extractor extract link từ HTML page.
7. Extracted link đi qua URL Filter.
8. Sau khi link được filter, chúng đi tới URL Seen kiểm tra xem URL này đã có trong URL storage chưa, chưa thì được add vào Frontier. Chưa thì discard.
# Design Deep Dive
## DFS vs BFS
BFS dùng FIFO thường được sử dụng bởi web crawler, tuy nhiên có 2 vấn đề với FIFO là:
- Hầu hết các link đều có chung một origin nên việc crawl nhiều link từ cùng 1 origin khiến cho server phải chịu tải nhiều => impolite.
![[Pasted image 20230425174517.png]]
- BFS không tính priority của URL.
## URL Frontier
