> Web crawler aka. robot or spider. Được dùng bởi các search engine để tìm ra content mới trên web. Content có thể là webpage, image, video, PDF file,... Web crawler sẽ collect một số pages rồi follow theo link ở trên nó tới page khác để collect content khác.
> ![[Pasted image 20230420162547.png]]
> Web crawler được dùng cho nhiều mục đích:
> - Search engine indexing: Tạo local index cho search engine (VD: Googlebot của Google).
> - Web archiving
> - Web mining: data mining (VD một số công ty lớn dùng crawler để download các cuộc họp hay báo cáo thường niên của cổ đông,...)

Basic algorithm cho web crawler là:
1. Cho một tập các URL, download webpage từ URL đó.
2. Extract URL từ các webpage này.
3. Tiếp tục add URL vào list và lặp lại bước 1.
> Requirement (từ trong sách)
> - Dùng cho search engine indexing
> - Mỗi tháng collect 1 tỷ page
> - Chỉ lưu file HTML
> - Consider cả những page mới thêm hoặc mới update
> - Lưu trữ trong vòng 5 năm
> - Ignore những page bị duplicate

Ngoài những requirement trên, ta còn phải tuân theo những tính chất của 1 good web crawler:
- Scalability: Vì có hàng tỷ webpage trên internet nên ta phải có cách crawling sao cho hiệu quả sử dụng ***parallelization***.
- Robustness: Những edge case có thể xảy ra: Bad HTML, unresponsive servers, crash, link độc hại,... Crawler phải handle được những case này.
- Politeness: Không nên tạo quá nhiều request trong một khoảng thời gian ngắn.
- Extensibility: Phải flexible.
> Back of envelope estimation
> - Giả sử có 1 tỷ webpage được tải mỗi tháng: QPS: 1 billion / 30 days / 24 hours / 3600 secs = 400 pages/second
> - Peak QPS = QPS * 2 (thường là vậy) = 800
> - Ví dụ size của mỗi page là 500k
> - 1 billion page * 100k = 500TB / month => 500 TB x 12 months x 5 years = 30 PB.

![[Pasted image 20230420171621.png]]
# High level design
## Seed URLs
Một web crawler sử dụng seed url làm *starting point*, một good seed url giúp ta có thể traverse qua càng nhiều link càng tốt.
## URL Frontier
Hầu hết modern web crawler chia crawl state thành 2 state: cần download và đã download. Component chứa URL cần download được gọi là **URL Frontier**. URL Frontier là 1 Queue.
## HTML Downloader
## DNS Resolver
## Content Parser
Sau khi download về, web page phải được parse và validate vì website bị malform sẽ gây tốn dung lượng. Content Parser nằm chung với crawl server có thể làm chậm quá trình crawling => Là 1 component riêng biệt.
## Content Seen?
Tránh lưu những content đã được lưu rồi.
## Content Storage
- Hầu hết data được lưu trên disk
- Popular content được lưu trong memory
## URL Extractor
URL Extract sẽ parse và extract link từ HTML pages
![[Pasted image 20230421151543.png]]
## URL Filter
URL Filter dùng để exclude ra những content type, file extension, error link, URL nằm trong *blacklist*.
## URL Seen?
Là 1 data structure keep track xem những URL nào được visit hoặc nằm trong Frontier rồi. Nó giúp ta tránh add cùng URL nhiều lần vì sẽ khiến tăng server load và có thể dẫn đến infinite loop.
*Bloom Filter* và *Hash table* là những technique phổ biến để implement "URL Seen" component.
## URL Storage
Lưu những URLs đã được visited.
# Web Crawler workflow
1. Add seed URLs vào URL Frontier.
2. HTML downloader fetch danh sách URL từ URL Frontier.
3. HTML Downloader get IP address của URL từ DNS resolver và bắt đầu download.
4. Sau khi Content Parser parses HTML page và kiểm tra xem page có bị malform không.
5. Content Seen component sẽ check xem HTML page đã có trong storage chưa. Rồi thì discard HTML page. Chưa thì được đẩy tiếp tới Link Extractor.
6. Link Extractor extract link từ HTML page.
7. Extracted link đi qua URL Filter.
8. Sau khi link được filter, chúng đi tới URL Seen kiểm tra xem URL này đã có trong URL storage chưa, chưa thì được add vào Frontier. Chưa thì discard.
# Design Deep Dive
## DFS vs BFS
BFS dùng FIFO thường được sử dụng bởi web crawler, tuy nhiên có 2 vấn đề với FIFO là:
- Hầu hết các link đều có chung một origin nên việc crawl nhiều link từ cùng 1 origin khiến cho server phải chịu tải nhiều => impolite.
![[Pasted image 20230425174517.png]]
- BFS không tính priority của URL.
## URL Frontier
URL frontier giúp ta giải quyết vấn đề trên. URL frontier là một data structure lưu URL sẽ được download. URL frontier là một component quan trọng vì nó đảm bảo *politeness*, *prioritization*, *freshness*.
### Politeness
Việc gửi quá nhiều request tới cùng một hosting server trong khoảng thời gian ngắn được coi là "impolite" và thậm chí sẽ được treated là DOS.
General idea là download mỗi lúc 1 page từ cùng host và có thể add thêm delay vào giữa 2 download tasks.
![[Pasted image 20230428151757.png]]
- **(b1, b2, b3):** là FIFO queue, mỗi queue sẽ chứa URL từ cùng một host.
- **Queue router:** đảm bảo b1, b2 và b3 chỉ chứa những URL tới từ 1 host.
- **Mapping table:** map host tới queue.
![[Pasted image 20230428152541.png]]
- **Queue Selector:** Một worker thread được map tới một FIFO queue và chỉ download URL từ queue đó. Queue selection logic sẽ được chọn bởi Queue selector.
- **Worker thread 1..N:** download webpage có URL lấy từ queue tương ứng.
### Priority
Chúng ta có thể priority URL dựa trên độ hữu ích của nó (theo PageRank, website traffic, update frequency). 
![[Pasted image 20230428152955.png]]
- **Prioritizer:** nhận vào URL và tính priority của nó.
- **Queue f1..fn:** Mỗi queue sẽ được gán cho một priority. Queue có priority cao thì sẽ có tỉ lệ được chọn cao hơn.
- **Queue selector:** Chọn ngẫu nhiên một queue nhưng thiên vị những queue có priority cao hơn.
Hình dưới  là URL frontier design gồm 2 modules: Front queues (quản lý priorization) và Back queues (quản lý politeness).
![[Pasted image 20230428153343.png]]
### Freshness
Web pages phải được crawl lại một cách định kỳ để đảm bảo downloaded page luôn là mới nhất. Recrawl toàn bộ URL thì rất tốn thời gian và tài nguyên => Ta có một số strategies để đảm bảo freshness như sau:
- Recrawl dựa theo webpage's update history.
- Prioritize URL và ưu tiên recrawl cũng như chọn tần suất recrawl dựa trên mức độ quan trọng.
### Storage cho URL Frontier
Real-world crawl cho search engine lưu trữ hàng trăm triệu URL trong frontier nên không thể lưu trong memory. Lưu vào disk thì lại chậm dễ dẫn đến bottleneck. => Hybrid approach: Phần lớn URL lưu trên disk => space không phải vấn đề. Cùng đó ta maintain một buffers trong memory cho enqueue và dequeue oprations. Data trong buffers sẽ định kỳ được ghi vào disk.
## HTML Downloader
HTML Downloader download webpage từ internet dùng HTTP protocol.
### Robots.txt
Hay còn gọi là *Robots exclusion protocol*, là một chuẩn được dùng bởi websites để giao tiếp với crawler. Nó chỉ rõ những page nào mà crawler được phép download. Trước khi crawl website thì crawler cần check file robots.txt trước và tuân theo quy định của nó.
### Perfomance Optimization
1. Distributed crawl: Để đạt được performance tốt nhất thì crawl jobs được distributed cho nhiều server, mỗi server chạy nhiều thread. URL space được partitioned thành nhiều pieces nhỏ => mỗi downloader lại phụ trách một subset của URL.
![[Pasted image 20230428155002.png]]
2. Cache DNS Resolver
> DNS Resolver is a bottleneck for crawlers because DNS requests might take time due to the synchronous nature of many DNS interfaces. DNS response time ranges from 10ms to 200ms. Once a request to DNS is carried out by a crawler thread, other threads are blocked until the first request is completed. Maintaining our DNS cache to avoid calling DNS frequently is an effective technique for speed optimization. Our DNS cache keeps the domain name to IP address mapping and is updated periodically by cron jobs.
3. Locality
Crawl server được phân tán về mặt địa lý => crawl server gần website host sẽ download nhanh hơn. Không chỉ áp dụng cho HTML Downloader mà còn cho các system component khác như: crawl servers, cache, queue, storage,...
4. Short timeout
Một số website respond rất chậm và nhiều khi còn không respond. Để tránh việc đợi quá lâu thì ta phải set maximal wait time.
### Robustness
- Consitent hashing: giúp distrubute load cho downloader.
- Save crawl state và data: để đối phó với failure/disruption và dễ dàng restart.
- Exception handling
- Data validation
### Extensibility
![[Pasted image 20230428160116.png]]
## Detect and avoid problematic content
1. Redundant content
30% webpage bị duplicate nên dùng hash hoặc checksum để detect duplication.
2. Spider traps
Spider trap là webpage khiến cho crawler rơi vào infinite loop.
3. Data noise
Tránh các content không có value như quảng cáo, code snippets, spam URL.
