Trong bài này ta được yêu cầu thiết kế key-value store hỗ trợ các operation sau:
- `put(key, value)`
- `get(key)`
Không có design nào là hoàn hào mà luôn có sự đánh đổi giữa
- *read*, *write* và *memory usage*.
- *tính toàn vẹn của dữ liệu* và *tính sẵn sàng*.
Trong bài này ta thiết kế key-value store có những đặc điểm sau:
- size của key-value pair nhỏ (*< 10 KB*).
- Có khả năng lưu big data.
- High availability (tốc độ phản hồi nhanh, kể cả khi đang xảy ra lỗi).
- High scalability (có thể scale để support những tập dữ liệu lớn.
- Automatic scaling (khi thêm/xóa servers).
- Tunable consistency.
- Độ trễ thấp.
# Single server key-value store
Nếu chỉ có 1 server thì cách tốt nhất là dùng hash-table lưu in-memory. Tuy tốc độ access nhanh nhưng space lại giới hạn nên ta có thể optimize như sau:
- Nén dữ liệu.
- Chỉ lưu data thường dùng trong memory còn lại lưu trên disk.
Tuy nhiên, server đơn cũng vẫn sẽ đạt đến giới hạn của nó một cách nhanh chóng nên ta cần 1 *distributed key-value store* để hỗ trợ big data.
# Distributed key-value store
## CAP theorem
- **Consistency** (tính nhất quán): consistency có nghĩa là tất cả client đều thấy data giống nhau tại cùng thời điểm bất kể họ connect tới node nào.
- **Availability** (tính khả dụng): Bất cứ client nào request data cũng nhận response về kể cả khi có node nào đó bị sập.
- **Partition Tolerance** (dung sai phân vùng): Partition ở đây là kết nối giữa 2 node bị break. Partition tolerance có nghĩa là hệ thống tiếp tục vận hành bất xảy ra partition trong hệ thống.
CAP theorem nói rằng: *"Phải hy sinh 1 trong 3 tính chất trên để có được 2 tính chất còn lại"*.
![[Pasted image 20230418081811.png]]
Key-value store thường được phân loại theo 2 tính chất CAP mà nó support => có 3 loại: **CP/AP/CA** system.
Bởi vì network failure là không thể tránh khỏi nên CA system không thể tồn tại khi áp dụng thực tiễn (có tồn tại nhưng nó là RDBMS - không phải distributed system).
## Ideal situation
Trong điều kiện lý tưởng thì network partition không bao giờ xảy ra => data viết vào `n1` sẽ tự đồng replicated qua `n2` và `n3`.
=> Đạt được cả C và A.
![[Pasted image 20230418083244.png]]
## Real-world distributed systems
![[Pasted image 20230418083549.png]]
Vì partition là không thể tránh khỏi nên ta phải chọn giữa Availability và Consistency.
Trong VD trên, khi `n3` down:
- Nếu chọn **C** thì chúng ta phải block tất cả write operation tới `n1` và `n2` để tránh inconsistency giữa 3 server nhưng lại làm cho nó unavailable.
- Nếu chọn **A** thì system thì read có thể trả về stale data. Đổi lại `n1` và `n2` vẫn accept write và data sẽ được sync cho `n3` khi network partition được khắc phục.
# System components
## Data partition
Có 2 vấn đề cần giải quyết khi phân vùng data:
- Phân bố data đồng đều cho các server.
- Giảm thiểu data movement khi thêm/bớt server.
Ta có thể dùng *consitent hashing* để giải quyết vấn đề trên + một số ưu điểm:
- **Automatic scaling**.
- **Heterogeneity:** the number of virtual nodes for a server is proportional to the server capacity. For example, servers with higher capacity are assigned with more virtual nodes.
## Data replication
Để đạt được *high availability* và *reliability*, data phải được async replicate cho N server với N là param được config. VD hình dưới là N = 3. `key0` được replicated tại `s1` `s2` và `s3`.
![[Pasted image 20230418084637.png]]
Nhưng nếu có virtual node thì N node đầu tiên có thể map tới ít hơn N physical servers. Để tránh issue này thì ta chỉ lấy unique server.
Node trong cùng data center thường fail với nhau (VD mất điện, network issues hay natural disaster, etc.) => Ta replicate ra các node nằm ở các data center khác nhau (được kết nối với nhau thống qua *high speed networks*) để tăng reliability.
## Consistency
Vì data được replicate ra nhiều node nên ta cần sync với nhau. **Quorum consensus** đảm bảo consistency cho cả *read* và *write* operation.
> - **N**: number of replicas
> - **W**: Write quorum có size W. Để write operation thành công thì operation phải được *acknowledged* từ W replicas.
> - **R**: Read quorum có size R. Để read operation thành công thì nó phải đợi *response* từ ít nhất R replica.

![[Pasted image 20230418085654.png]]
 *coordinator* hoạt động như một Proxy giữa client và node. Write operation có W = 1 **không** có nghĩa là data chỉ lưu ở 1 node mà là coordinator chỉ xem như operation đó thành công nếu nhận được ít nhất *1 acknowledgment*. Còn data vẫn sẽ replicate ra toàn bộ N node.
 Việc chọn *W, R và N* thường phải đánh đổi giữa latency và consistency. Nếu W = 1 hoặc R = 1 thì operation sẽ returned nhanh chóng vì coordinator chỉ cần chờ 1 response từ bất cứ replica nào. Nếu > 2 thì query sẽ chậm hơn nhưng lại đảm bảo consistency tốt hơn.
 > Nếu *W + R > N* thì strong consistency được đảm bảo vì có ít nhất 1 node chứa latest data.
 > Một số setup khả thi:
 > - R = 1 và W = N: optimize for fast read
 > - W = 1 và R = N: optimize for fast write
 > - W + R > N, strong consistency (thường là N = 3, W = R = 2)
 > - W + R < N, strong consistency không được đảm bảo
 
## Consistency models
Là mức consistency của data:
- **Strong**: Luôn read được data mới nhất (client không bao giờ đọc được data cũ).
- **Weak**: Subsequent read operation có thể không thấy được data mới nhất.
- **Eventual**: Là 1 dạng của weak. Sau 1 khoảng thời gian đủ thì mới update xong ở mọi replicas.
Thường chỉ đạt được mức Strong khi không cho các replica accept read/write operation mới tới khi mọi replica đều đã accept operation write hiện tại (đang write thì không cho read/write mới). Nhưng dẫn đến availability thấp vì nó block new operation.
Eventual là recommend model.
## Inconsistency resolution: versioning
Replication giúp tăng availability nhưng gây ra inconsistency => dùng *versioning* và *vector lock*.
![[Pasted image 20230418103301.png]]
![[Pasted image 20230418103325.png]]
=> Xảy ra conflict giữa version cuối cùng, ta cần một versioning system để *detect và reconcile* conflict: **VECTOR CLOCK**.
